
# 项目说明

本项目包含三部分：BERT 路径优化及改进、FastText 有监督训练及自迭代，以及 LCCC 数据集的清洗流程。各部分的具体内容如下：

---

## （一）BERT

### 1. 路径优化
- 从 `file_path` 中提取并保存 BERT 标记好的文件，确保生成的文件名不与原文件重名。
- 文件存储路径为 `../data/ft_data/labelled`，所有文件合并为 `all_labelled.txt`。
- 在合并时从数据集中抽取贴吧名和数据来源，规则如下：
  - 贴吧名提取：如 `'tp_yuanshen_tp_ft_trunc500000_labeled'`，贴吧名为 `'yuanshen'`。
  - 数据来源提取：如 `'tp_yuanshen_tp_ft_trunc500000_labeled'`，来源为 `'tp'`。
- 合并后数据集新增两列 `tieba_name` 和 `source`，最终生成的文件与 `all_labelled.txt` 列数一致，共六列。

### 2. 工程改进
- 当前工程有三个主要任务流程：
  1. BERT 数据初始化。
  2. 加入断点处理逻辑。
  3. 优化自迭代超参数，以改善标签分布未见明显变化的问题。
  
- 改进建议：
  - 增加 `FastText` 有监督训练和自迭代过程。
  - 数据加载优化：使用 `Dask` 以降低内存占用，将 `all_labelled.txt` 压缩后加载。
  - 参数微调建议：
    1. 动态调整 NVIDIA 4070 显存调度。
    2. 部署模型剪枝和量化，加速 NSP 模型推理。
    3. 调整 BERT NSP 模型学习率，混合使用 `MinMaxScaler` 和 `GradScaler`。

### 3. 人工标注
- 可视化生成结果。当前有两个人工标注文件，每个文件约 200 行。
- 使用训练完成的 `FastText` 生成第七列 `related_ft`，值区间为 [0,1]。
- 加入主观判断，生成第八列 `related_classes_ft`。
- 最终表格包括以下列：`tieba_name`, `source`, `ask_content`, `answer_content`, `related`, `related_classes`。

---

## （二）FastText

### 1. 有监督训练及自迭代
- 确保 BERT 标记完成的数据可被 `FastText` 识别，并用于训练。
- 每轮自迭代随机采样生成负样本，将新生成的数据与原数据集一起进行训练。

### 2. 分词
- 没有对 `ask_content` 和 `answer_content` 进行分词处理，导致每轮迭代数据量未变。
- 评估分词质量，结合多种 NLP 指标优化分词方案。

### 3. 作图
- 将回归值按 0-1 区间划分为 10 个区段（步长为 0.1），并生成柱状图显示样本数量。不同区间样本用不同颜色区分。

### 4. 自迭代作图
- 横轴为贴吧名，纵轴为最大标签样本数。
- 保存自迭代中两极化最明显的一轮和最后一轮的结果至 `logs/` 文件夹。

### 5. 数据样本留存率
- 计算并显示数据样本的留存率。假设初始数据为 7,142,856 行，留存数据为 4,285,719 行，则留存率为 `4285719/7142856`。

---

## （三）LCCC 数据集

### 1. 数据清洗流程
- 过滤广告及无关内容，移除 URL 和平台标记，确保文本只含对话内容。
- 检测并合并重复对话，清除异常对话。
- 手动标注提升数据集准确性。

---

## 文件结构
```
|-- data/
|   |-- ft_data/
|   |   |-- labelled/
|   |   |   |-- all_labelled.txt
|-- logs/
|   |-- iteration_logs.txt
```

---

## 使用说明

1. **BERT 路径优化及处理**：运行脚本对 BERT 生成的文件进行标注和合并处理。
2. **FastText 训练**：确保所有标记数据可以被 `FastText` 识别，并进行自迭代负样本生成和训练。
3. **LCCC 数据清洗**：执行数据清洗流程，确保数据集只包含有意义的对话内容。
